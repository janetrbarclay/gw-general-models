{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create MODFLOW-grid-based tiff files and model_grid.csv file from GIS data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project specific variables are imported in the model_spec.py and gen_mod_dict.py files that must be included in the notebook directory. The first first includes pathnames to data sources that will be different for each user. The second file includes a dictionary of model-specific information such as cell size, default hydraulic parameter values, and scenario defintion (e.g. include bedrock, number of layers, etc.). There are examples in the repository. Run the following cells up to the \"Run to here\" cell to get a pull-down menu of models in the model_dict. Then, without re-running that cell, run all the remaining cells. Re-running the following cell would re-set the model to the first one in the list, which you probably don't want. If you use the notebook option to run all cells below, it runs the cell you're in, so if you use that option, move to the next cell (below the pull-down menu of models) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'Jeff Starn'\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "import scipy.spatial as ss\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import flopy as fp\n",
    "from flopy.utils.reference import SpatialReference\n",
    "\n",
    "import geopandas as gp\n",
    "import mplleaflet\n",
    "import time\n",
    "from shapely.geometry import box as shape_box\n",
    "from shapely.geometry import Polygon, LineString\n",
    "\n",
    "from rtree import index\n",
    "\n",
    "from copy import deepcopy\n",
    "import gdal\n",
    "gdal.UseExceptions()\n",
    "import ogr\n",
    "import osr\n",
    "import pandas as pd\n",
    "from model_specs import *\n",
    "from gen_mod_dict import *\n",
    "pth = 'MFGrid/mfgrid'\n",
    "sys.path.append(pth)\n",
    "import grid as modgrid\n",
    "\n",
    "from ipywidgets import interact, Dropdown\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell doesn't do anything in the notebook. It is the template\n",
    "for turning the notebook into a batch python script. To run in batch mode, download this notebook as a python script and place the entire body of the script in place of the \"pass\" command after the \"try\" statement. Make sure the indent level is the same as \"pass\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying Niantic_test\n",
      "trying Niantic250\n",
      "trying Niantic\n",
      "trying Niantic500\n",
      "trying CoastalCT250\n",
      "trying Neversink\n",
      "trying CoastalCT500\n",
      "trying Assabet\n",
      "trying CoastalCT\n"
     ]
    }
   ],
   "source": [
    "for key, value in model_dict.items():   # from \\\"gen_mod_dict.py\\\"\\n\",\n",
    "    md = key\n",
    "    ms = model_dict[md]\n",
    "    print('trying {}'.format(md))\n",
    "    try:\n",
    "        pass\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = list(model_dict.keys())\n",
    "models.sort()\n",
    "model_area = Dropdown(\n",
    "    options=models,\n",
    "    description='Model:',\n",
    "    background_color='cyan',\n",
    "    border_color='black',\n",
    "    border_width=2)\n",
    "display(model_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run to here to initiate notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time using this notebook in this session (before restarting the notebook), run the cells up to this point. Then select your model from the dropdown list above. Move your cursor to this cell and use the toolbar menu Cell --> Run All Below.  After the first time, if you want to run another model, select your model and start running from this cell--you don't need to re-run the cells from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model being processed is Niantic_test\n"
     ]
    }
   ],
   "source": [
    "md = model_area.value\n",
    "ms = model_dict[md]\n",
    "print('The model being processed is {}'.format(md))\n",
    "\n",
    "if 'L' in ms.keys():\n",
    "    L = ms['L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/workspace/gw-general-models/Genmod1.0/subprojects/siteGeneral'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_dir   # Pathnames from \"model_specs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ib_filter = ms['ib_filter']\n",
    "\n",
    "model_ws = os.path.join(proj_dir, ms['ws'])   # values from \"gen_mod_dict.py\"\n",
    "nhd_basin_dir = ms['vpu']\n",
    "rpu = ms['rpu']\n",
    "domain_file = os.path.join(model_ws, ms['df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pathnames for the NHD and Glacial Texture map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/workspace/gw-general-models/Genmod1.0/input_data/NHDPlusV2Data\\NHDPlusNE_MA/NHDPlus01_02\\NHDSnapshot\\Hydrography\\NHDFlowline.shp\n",
      "C:/workspace/gw-general-models/Genmod1.0/input_data/NHDPlusV2Data\\NHDPlusNE_MA/NHDPlus01_02\\NHDSnapshot\\Hydrography\\NHDWaterbody.shp\n",
      "C:/workspace/gw-general-models/Genmod1.0/input_data/NHDPlusV2Data\\NHDPlusNE_MA/NHDPlus01_02\\NHDPlusAttributes\\PlusFlowlineVAA.dbf\n",
      "C:/workspace/gw-general-models/Genmod1.0/input_data/NHDPlusV2Data\\NHDPlusNE_MA/NHDPlus01_02\\NHDSnapshot\\NHDFCode.dbf\n",
      "C:/workspace/gw-general-models/Genmod1.0/input_data/NHDPlusV2Data\\NHDPlusNE_MA/NHDPlus01_02\\NHDPlusAttributes\\ElevSlope.dbf\n"
     ]
    }
   ],
   "source": [
    "# Pathnames from \"model_specs.py\"\n",
    "flow_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDSnapshot', 'Hydrography', 'NHDFlowline.shp')\n",
    "lake_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDSnapshot', 'Hydrography', 'NHDWaterbody.shp')\n",
    "VAA_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusAttributes', 'PlusFlowlineVAA.dbf')\n",
    "fcode_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDSnapshot', 'NHDFCode.dbf')\n",
    "slope_file = os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusAttributes', 'ElevSlope.dbf')\n",
    "\n",
    "#stack_file = os.path.join(geol_dir, 'factor_added_Stack_map.shp')\n",
    "# subsurf_file = os.path.join(geol_dir, 'Subsurface_(Selected_Areas).shp')\n",
    "# surfmat_file = os.path.join(geol_dir, 'Surficial_Materials.shp')\n",
    "# veneer_file = os.path.join(geol_dir, 'Veneer_(Selected_Areas).shp')\n",
    "\n",
    "print(flow_file)\n",
    "print(lake_file)\n",
    "print(VAA_file)\n",
    "print(fcode_file)\n",
    "print(slope_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_raster_data(src, method, conversion=1.0):\n",
    "    '''\n",
    "    Takes a raster data source (ESRI grid, GeoTiff, .IMG and many other formats)\n",
    "    and returns a numpy array. Arrangment of pixels is given as input and may \n",
    "    correspond to a MODFLOW grid.\n",
    "    \n",
    "    src : string\n",
    "        complete path to raster data source\n",
    "    method : string\n",
    "        gdal method for interpolation. Choices are:\n",
    "            gdal.GRA_NearestNeighbour \n",
    "                Nearest neighbour (select on one input pixel)\n",
    "            gdal.GRA_Bilinear\n",
    "                Bilinear (2x2 kernel)\n",
    "            gdal.GRA_Cubic\n",
    "                Cubic Convolution Approximation (4x4 kernel)\n",
    "            gdal.GRA_CubicSpline\n",
    "                Cubic B-Spline Approximation (4x4 kernel)\n",
    "            gdal.GRA_Lanczos\n",
    "                Lanczos windowed sinc interpolation (6x6 kernel)\n",
    "            gdal.GRA_Average\n",
    "                Average (computes the average of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Mode\n",
    "                Mode (selects the value which appears most often of all the sampled points)\n",
    "            gdal.GRA_Max\n",
    "                Max (selects maximum of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Min\n",
    "                Min (selects minimum of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Med\n",
    "                Med (selects median of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Q1\n",
    "                Q1 (selects first quartile of all non-NODATA contributing pixels)\n",
    "            gdal.GRA_Q3\n",
    "                Q3 (selects third quartile of all non-NODATA contributing pixels)\n",
    "\n",
    "    conversion : float\n",
    "        factor to be applied to raw data values to change units\n",
    "\n",
    "    requires global variables (for now):\n",
    "    NCOL, NROW : number of rows and columns\n",
    "    gt : geotransform list\n",
    "    shapeproj : coordinate reference system of NHDPlus (or other desired projection)\n",
    "    hnoflo : to be used as missing data value (from model_spec.py)\n",
    "\n",
    "    returns:\n",
    "    2D array of raster data source projected onto model grid. \n",
    "    Returns a zero array with the correct shape if the source does not exist.\n",
    "    '''\n",
    "    if os.path.exists(src):\n",
    "        rast = gdal.Open(src)\n",
    "\n",
    "        dest = make_grid(NCOL, NROW, gt, shapeproj)\n",
    "        gdal.ReprojectImage(rast, dest, rast.GetProjection(), shapeproj, method)\n",
    "\n",
    "        grid = dest.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "        grid = grid * conversion\n",
    "\n",
    "        dest = None\n",
    "        rast = None\n",
    "    else:\n",
    "        grid = np.ones((NROW, NCOL)) * hnoflo\n",
    "        print('Data not processed for\\n{}\\n Check that the file exists and path is correct'.format(src))\n",
    "\n",
    "    return grid\n",
    "\n",
    "def process_vector_data(src, attribute, **kwargs):\n",
    "    '''\n",
    "    Takes a vector data source (ESRI shapefile) and returns a numpy array.\n",
    "    Arrangment of pixels is given as input and may correspond to a MODFLOW grid.\n",
    "\n",
    "    src : complete path to vector data source\n",
    "    attribute : field in data table to assign to rasterized pixels\n",
    "    \n",
    "    requires global variables:\n",
    "    NCOL, NROW : number of rows and columns\n",
    "    gt : geotransform list\n",
    "    shapeproj : coordinate reference system of NHDPlus\n",
    "    hnoflo : to be used as missing data value (from model_spec.py)\n",
    "    \n",
    "    returns:\n",
    "    2D array of vector data source projected onto model grid.\n",
    "    Returns a zero array with the correct shape if the source does not exist.\n",
    "    '''\n",
    "    if os.path.exists(src):\n",
    "\n",
    "        datasource = ogr.Open(src)\n",
    "        layer = datasource.GetLayer()\n",
    "\n",
    "        src = make_grid(NCOL, NROW, gt, shapeproj, 0)\n",
    "        args = ['ATTRIBUTE={}'.format(attribute)]\n",
    "        if 'options' in kwargs.keys():\n",
    "            for i in kwargs['options'] :\n",
    "                args.append(i)\n",
    "        gdal.RasterizeLayer(src, [1], layer, options = args)\n",
    "\n",
    "        grid = src.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "        src = None\n",
    "        dst = None        \n",
    "    else:\n",
    "        grid = np.ones((NROW, NCOL)) * hnoflo\n",
    "        print('Data not processed for\\n{}\\n Check that the file exists and path is correct'.format(src))\n",
    "\n",
    "    return grid\n",
    "\n",
    "def make_raster(dst_file, data, NCOL, NROW, gt, proj, nodata):\n",
    "    '''\n",
    "    Writes numpy array to a GeoTiff file.\n",
    "    \n",
    "    dst_file : name of file to write\n",
    "    data : 2D numpy array\n",
    "    NCOL, NROW : number of rows and columns. These may coincide with a MODFLOW grid.\n",
    "    gt : 6-element geotransform list [C, A, B, F, E, D]. Gives the coordinates of one pixel\n",
    "        (the upper left pixel). If there is no rotation, B=D=0. If cells are square, A=-E.   \n",
    "        Letter designations come from the original documentation.\n",
    "        \n",
    "        C = x coordinate in map units of the upper left corner of the upper left pixel\n",
    "        A = distance from C along x axis to upper right pixel corner of the upper left pixel\n",
    "        B = distance from C along x axis to lower left pixel corner of the upper left pixel,\n",
    "        F = y coordinate in map units of the upper left corner of the upper left pixel\n",
    "        E = distance from C along y axis to lower left pixel corner of the upper left pixel\n",
    "        D = distance from C along y axis to upper right pixel corner of the upper left pixel\n",
    "        \n",
    "    proj : projection of the GeoTiff\n",
    "    nodata : value to use as missing data in the GeoTiff\n",
    "    '''\n",
    "    import gdal\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst = driver.Create(dst_file, NCOL, NROW, 1, gdal.GDT_Float32)\n",
    "    dst.SetGeoTransform(gt)\n",
    "    dst.SetProjection(proj)\n",
    "    band = dst.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata)\n",
    "    band.WriteArray(data)\n",
    "    dst = None\n",
    "\n",
    "def make_grid(NCOL, NROW, gt, proj, nodata=hnoflo):\n",
    "    '''\n",
    "    Creates a blank raster image in memory.\n",
    "        \n",
    "    NCOL, NROW : number of rows and columns. These may coincide with a MODFLOW grid.\n",
    "    gt : 6-element geotransform list [C, A, B, F, E, D]. Gives the coordinates of one pixel\n",
    "        (the upper left pixel). If there is no rotation, B=D=0. If cells are square, A=-E.   \n",
    "        Letter designations come from the original documentation.\n",
    "        \n",
    "        C = x coordinate in map units of the upper left corner of the upper left pixel\n",
    "        A = distance from C along x axis to upper right pixel corner of the upper left pixel\n",
    "        B = distance from C along x axis to lower left pixel corner of the upper left pixel,\n",
    "        F = y coordinate in map units of the upper left corner of the upper left pixel\n",
    "        E = distance from C along y axis to lower left pixel corner of the upper left pixel\n",
    "        D = distance from C along y axis to upper right pixel corner of the upper left pixel\n",
    "        \n",
    "    proj : projection of the GeoTiff\n",
    "    nodata : value to use as missing data in the GeoTiff\n",
    "    '''\n",
    "    import gdal\n",
    "    mem_drv = gdal.GetDriverByName('MEM')\n",
    "    grid_ras = mem_drv.Create('', NCOL, NROW, 1, gdal.GDT_Float32)\n",
    "    grid_ras.SetGeoTransform(gt)\n",
    "    grid_ras.SetProjection(shapeproj)\n",
    "    band = grid_ras.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata)\n",
    "    array = np.zeros((NROW,NCOL))\n",
    "    band.WriteArray(array)\n",
    "    return grid_ras\n",
    "\n",
    "def process_mohp_data(tif_files):\n",
    "    '''\n",
    "    Loops a list of MOHP tif files. The rest of the algorithm is similar to the function\n",
    "    \"process_raster_data\" except that a transformation from the ESRI WKT format to a generic\n",
    "    format is needed. When MOHP data source is finalized, this function can be modified\n",
    "    to work with the final format.\n",
    "    \n",
    "    src : complete path to raster data source\n",
    "    method : gdal method for interpolation\n",
    "    conversion : factor to be applied to raw data values to change units\n",
    "\n",
    "    requires global variables (for now):\n",
    "    NCOL, NROW : number of rows and columns\n",
    "    gt : geotransform list\n",
    "    shapeproj : coordinate reference system of NHDPlus (or other desired projection)\n",
    "    hnoflo : to be used as missing data value (from model_spec.py)\n",
    "\n",
    "    returns:\n",
    "    2D array of raster data source projected onto model grid. Each column contains\n",
    "    a different stream order MOHP. Each row corresponds to a model cell. \n",
    "    Number of rows is NCOL x NCOL. Number of columns is number of stream orders present.\n",
    "    '''\n",
    "    import gdal\n",
    "    gdal.UseExceptions()\n",
    "    import ogr\n",
    "    import osr\n",
    "    \n",
    "    arr = np.zeros((NCOL * NROW, len(tif_files)))\n",
    "    if tif_files != []:\n",
    "        for col, src in enumerate(tif_files):\n",
    "            hp = gdal.Open(src)\n",
    "\n",
    "            dest = make_grid(NCOL, NROW, gt, shapeproj)\n",
    "\n",
    "            srs = osr.SpatialReference()\n",
    "            srs.ImportFromWkt(hp.GetProjection())\n",
    "            srs.MorphFromESRI()\n",
    "            hp_prj = srs.ExportToWkt()\n",
    "            hp.SetProjection(hp_prj)\n",
    "\n",
    "            gdal.ReprojectImage(hp, dest, hp.GetProjection(), shapeproj, gdal.GRA_NearestNeighbour)\n",
    "\n",
    "            hp_grd = dest.GetRasterBand(1).ReadAsArray()\n",
    "            hp_grd = hp_grd / 10000.\n",
    "\n",
    "            dst = None\n",
    "            hp = None\n",
    "            \n",
    "            arr[:, col] = hp_grd.ravel()\n",
    "    return arr\n",
    "\n",
    "def make_clockwise(coords):\n",
    "    '''\n",
    "    Function to determine direction of vertices of a polygon (clockwise or CCW).\n",
    "    Probably not needed, but here just in case. \n",
    "    \n",
    "    coords : array with dim (n, 2)\n",
    "            n is number of vertices in the polygon. The last vertex is the same \n",
    "            as the first to close the polygon. The first column is x and the second is y.\n",
    "    '''\n",
    "    # if the points are counterclockwise, reverse them\n",
    "    x1 = coords[:-1, 0]\n",
    "    x2 = coords[1:, 0]\n",
    "    y1 = coords[:-1, 1]\n",
    "    y2 = coords[1:, 1]\n",
    "    ccw = np.sum((x2 - x1) * (y2 + y1)) < 0\n",
    "    if ccw:\n",
    "        coords = np.flipud(coords)\n",
    "        print('yup, coordinates are ccw')\n",
    "        print(\"let's change them to CW\")\n",
    "    return coords\n",
    "\n",
    "# test data for make_clockwise\n",
    "\n",
    "# print('clockwise')\n",
    "# x = np.array([1, 1, 2, 2, 1])\n",
    "# y = np.array([1, 2, 2, 1, 1])\n",
    "# coords = np.array(zip(x, y))\n",
    "# c = make_clockwise(coords)\n",
    "# print( c)\n",
    "# print('\\n')\n",
    "# print('CCW')\n",
    "# x = np.array([1, 2, 2, 1, 1])\n",
    "# y = np.array([1, 1, 2, 2, 1])\n",
    "# coords = np.array(zip(x, y))\n",
    "# c = make_clockwise(coords)\n",
    "# print( c)\n",
    "\n",
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from shutil import copyfile\n",
    "\n",
    "def dbf2df(dbf_path, index=None, cols=False, incl_index=False):\n",
    "    '''\n",
    "    Read a dbf file as a pandas.DataFrame, optionally selecting the index\n",
    "    variable and which columns are to be loaded.\n",
    "\n",
    "    __author__  = \"Dani Arribas-Bel <darribas@asu.edu> \"\n",
    "    ...\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dbf_path    : str\n",
    "                  Path to the DBF file to be read\n",
    "    index       : str\n",
    "                  Name of the column to be used as the index of the DataFrame\n",
    "    cols        : list\n",
    "                  List with the names of the columns to be read into the\n",
    "                  DataFrame. Defaults to False, which reads the whole dbf\n",
    "    incl_index  : Boolean\n",
    "                  If True index is included in the DataFrame as a\n",
    "                  column too. Defaults to False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df          : DataFrame\n",
    "                  pandas.DataFrame object created\n",
    "    '''\n",
    "    db = ps.open(dbf_path)\n",
    "    if cols:\n",
    "        if incl_index:\n",
    "            cols.append(index)\n",
    "        vars_to_read = cols\n",
    "    else:\n",
    "        vars_to_read = db.header\n",
    "    data = dict([(var, db.by_col(var)) for var in vars_to_read])\n",
    "    if index:\n",
    "        index = db.by_col(index)\n",
    "        db.close()\n",
    "        return pd.DataFrame(data, index=index)\n",
    "    else:\n",
    "        db.close()\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "def fill(data, invalid=None):\n",
    "    #from https://stackoverflow.com/questions/5551286/filling-gaps-in-a-numpy-array\n",
    "    #uses a nearest neighbor interpolation\n",
    "    \"\"\"\n",
    "    Replace the value of invalid 'data' cells (indicated by 'invalid') \n",
    "    by the value of the nearest valid data cell\n",
    "\n",
    "    Input:\n",
    "        data:    numpy array of any dimension\n",
    "        invalid: a binary array of same shape as 'data'. True cells set where data\n",
    "                 value should be replaced.\n",
    "                 If None (default), use: invalid  = np.isnan(data)\n",
    "\n",
    "    Output: \n",
    "        Return a filled array. \n",
    "    \"\"\"\n",
    "    #import numpy as np\n",
    "    #import scipy.ndimage as nd\n",
    "\n",
    "    if invalid is None: invalid = np.isnan(data)\n",
    "\n",
    "    ind = nd.distance_transform_edt(invalid, return_distances=False, return_indices=True)\n",
    "    return data[tuple(ind)]\n",
    "\n",
    "def interp_grid(a, naVal = 0):\n",
    "    a[a==naVal]=np.nan\n",
    "    x, y = np.indices(a.shape)\n",
    "    interp = np.array(a)\n",
    "    interp[np.isnan(interp)] = griddata( (x[~np.isnan(a)], y[~np.isnan(a)]), a[~np.isnan(a)], (x[np.isnan(a)], y[np.isnan(a)]))\n",
    "    interp[np.isnan(interp)] = naVal\n",
    "    return interp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate model grid coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the steps for creating and populating the MODFLOW grid in GIS space is as follows:\n",
    "\n",
    "* Read the domain shapefile and store its projection. The shapefile containing outline of model domain must have an attribute called \"ibound.\"\n",
    "* Make the convex hull of the model domain and extract the coordinates of the hull.\n",
    "* Iterate around the sides of the convex hull, rotating it so that each iteration the next side is parallel to the x axis.\n",
    "* Make a bounding box around the rotated hull and store its area.\n",
    "* The angle of rotation that corresponds to the bounding box with the smallest area is the rotation that produces the smallest number of inactive cells.\n",
    "* Make a geotransform list so that any arbitrary data can be projected on to the model grid in GIS space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain_diss = gp.read_file(domain_file)\n",
    "prj = domain_diss.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the NED grid from NHDPlus to get the projection. NHDPlus lines are in geographic coordinates rather than a projected coordinate reference system. Extract the projection as WKT, convert it to Proj.4 format, then parse that into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "ned = gdal.Open(src)\n",
    "shapeproj = ned.GetProjection()\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromWkt(shapeproj)\n",
    "prj4 = (srs.ExportToProj4()).split('+')\n",
    "prj4 = dict([item.split('=') for item in prj4 if len(item.split('=')) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_diss.to_crs(prj4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_diss.ibound = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# domain_diss.plot()\n",
    "domain_diss.to_file(os.path.join(model_ws, 'domain_outline.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following lines to generate a leaflet map of the model domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ax = domain_diss.plot()\n",
    "#mplleaflet.display(fig=ax.figure, crs=prj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find convex hull and extract the coordinates of its vertices. (Add the function to change to clockwise here if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hull = domain_diss.convex_hull\n",
    "coords2 = np.array(hull[0].exterior.coords[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the domain (dc) and angle (da) of each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc2 = np.diff(coords2, axis=0)\n",
    "da2 = np.arctan2(dc2[:,1], dc2[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate to each side of the convex hull and calculate the area of each bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vert = da2.shape[0]\n",
    "area2 = np.zeros((num_vert))\n",
    "for i in range(num_vert):\n",
    "    hull_rot = hull.rotate(-da2[i], origin=tuple(coords2[i, :]), use_radians=True)\n",
    "    minx, miny, maxx, maxy = hull_rot.bounds.iloc[0]\n",
    "    area2[i] = (maxx - minx) * (maxy - miny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a rectangular polygon and geoseries corresponding to the minimum area bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ii2 = np.argmin(area2)\n",
    "hull_rot = hull.rotate(-da2[ii2], origin=tuple(coords2[ii2, :]), use_radians=True)\n",
    "# minx, miny, maxx, maxy = hull_rot.bounds.iloc[0]\n",
    "# temp_poly = shape_box(minx, miny, maxx, maxy, ccw=False)\n",
    "# temp_gdf = gp.GeoSeries(temp_poly)\n",
    "temp_gdf = hull_rot.envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate the bounding box back to its original coordinate framework and set its projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box = temp_gdf.rotate(da2[ii2], origin=tuple(coords2[ii2, :]), use_radians=True)\n",
    "box.crs = prj4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box.to_file(os.path.join(model_ws, 'clip_box.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the geometry for use later in clipping NHD flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = box.geometry[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a visualing pleasing origin and use it as the model origin. \n",
    "\n",
    "Extract the coordinates of the box; each line is an (x, y) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yup, coordinates are ccw\n",
      "let's change them to CW\n"
     ]
    }
   ],
   "source": [
    "pts = np.array(box[0].exterior.coords[:])\n",
    "pts = make_clockwise(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last vertices of a polygon are identical, so strip off the last vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pts = pts[:-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the line containing the coordinates of the apex (ymax) of the hull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ymax = np.argmax(pts[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap (roll) the lines of the array around so that the apex is at the top of the array (first line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pts = np.roll(pts, -ymax, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the first point (ymax) back to the last position to complete the polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pts = np.vstack((pts, pts[0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the domain (dc) and angle (da) of each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = np.diff(pts, axis=0)\n",
    "da = np.arctan2(dc[:, 1], dc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine shape of bounding box to determine row and column directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dx, dy = np.amax(dc, axis=0)\n",
    "# is_tall = dx < dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the geotransform list to be used for creating raster images later. See the function \"make_raster\" for an explanation. Theta is not used to make rasters; it's used to rotate NHD flowlines.\n",
    "\n",
    "The orientation of the top side of the model grid is determined by the angle from (x, ymax) to the next vertex to its right in a clockwise direction:\n",
    "\n",
    "* If the angle is < -45 degrees (-pi/4), the top side is to the left of (x, ymax) and the origin is (x, ymax - 1)\n",
    "    \n",
    "* If the angle is >= -45 degrees (-pi/4), the top side is to the right of (x, ymax) and the origin is (x, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "side_lengths = np.hypot(dc[:, 0], dc[:, 1])\n",
    "\n",
    "if da[ymax] < (np.pi / -4):\n",
    "    or_cor = ymax - 1\n",
    "else:\n",
    "    or_cor = ymax\n",
    "\n",
    "theta = da[or_cor]\n",
    "origin = pts[or_cor, :]\n",
    "x_len = side_lengths[or_cor]\n",
    "y_len = side_lengths[or_cor - 1]\n",
    "\n",
    "if md.startswith(\"CoastalCT\"):\n",
    "    origin = pts[or_cor-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of rows and columns for the L given in model_spec.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NROW = np.int(np.ceil(y_len / L))\n",
    "NCOL = np.int(np.ceil(x_len / L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 24\n"
     ]
    }
   ],
   "source": [
    "print (NROW, NCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the number of rows and columns with those from an existing model if one is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = ms.keys()\n",
    "if 'NROW' in keys:\n",
    "    NROW = ms['NROW']\n",
    "    NCOL = ms['NCOL']\n",
    "    LX = x_len / NCOL\n",
    "    LY = y_len / NROW\n",
    "else:\n",
    "    LX = L\n",
    "    LY = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = LX * np.cos(theta)\n",
    "B = LY * np.sin(theta)\n",
    "D = LX * np.sin(theta)\n",
    "E = LY * -np.cos(theta)\n",
    "\n",
    "gt = [origin[0], A, B, origin[1], D, E]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a file of information that summarizes all of the above stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_spec_file = os.path.join(model_ws, 'grid_spec.txt')\n",
    "with open(grid_spec_file, 'w') as f:\n",
    "    line = 'Source locations\\n'\n",
    "    f.write(line)\n",
    "    for key, val in ms.items():\n",
    "        f.write('  {} : {}\\n'.format(key, val))\n",
    "    line = '\\nUpper left corner x and y projected coordinates\\n'\n",
    "    f.write(line)\n",
    "    line = '{} {}\\n\\n'.format(origin[0], origin[1])\n",
    "    f.write(line)\n",
    "    line = 'Rotation about upper left corner in radians and degrees from positive x axis\\n'\n",
    "    f.write(line)\n",
    "    line = '{} {}\\n\\n'.format(theta, theta * 180 / np.pi)\n",
    "    f.write(line)\n",
    "    line = 'Grid corner projected coordinates\\n'\n",
    "    f.write(line)\n",
    "    for row in pts:\n",
    "        f.write(' {:0.2f}  {:0.2f} '.format(*row))\n",
    "    line = '\\nCoordinate reference system\\n'\n",
    "    f.write(line)\n",
    "    for key, val in prj4.items():\n",
    "        f.write('    {} : {}\\n'.format(key, val))\n",
    "    line = '\\n\\nGeotransformation block\\n'\n",
    "    f.write(line)\n",
    "    line = '{} {} {} {} {} {}'.format(*gt)\n",
    "    f.write(line)\n",
    "    line = '\\n\\nGrid block\\n'\n",
    "    f.write(line)\n",
    "    line = 'NROW {} NCOL {}'.format(NROW, NCOL)\n",
    "    f.write(line)\n",
    "    line = '\\n\\nCell Size\\n'\n",
    "    f.write(line)\n",
    "    line = '{} '.format(L)\n",
    "    f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process river flowlines and lake polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read HD flowlines and associated tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flowlines = gp.read_file(flow_file)\n",
    "VAA_tab = dbf2df(VAA_file, cols=['ComID', 'StreamOrde','ArbolateSu','Hydroseq','ToNode','FromNode'])\n",
    "slope_tab = dbf2df(slope_file, cols=['COMID', 'MAXELEVSMO', 'MINELEVSMO'])\n",
    "fcode_tab = dbf2df(fcode_file, cols=['FCode', 'Hydrograph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract projection information, which for NHDPlus flowlines will be geographic (lat long) coordinates. Project grid box outline to the NHD (geographic) coordinates. Clip the NHD to the box and project it back to the projected coordinate system (probably Albers). Note: the crs from NHD is given below as epsg code 4269.  This is the same as a plain NAD83 crs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latlong_prj = flowlines.crs\n",
    "latlong_box = box.to_crs(crs=latlong_prj)\n",
    "latlong_clip = latlong_box.geometry[0]\n",
    "lines = flowlines['geometry'].intersection(latlong_clip)\n",
    "lines = lines.to_crs(crs=prj4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add fields from the NHD back to the clipped lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = pd.DataFrame(lines, columns=['geometry'])\n",
    "lines['COMID'] = flowlines.COMID\n",
    "lines['FCODE'] = flowlines.FCODE\n",
    "lines['REACHCODE'] = flowlines.REACHCODE\n",
    "lines['LENGTHKM'] = flowlines.LENGTHKM\n",
    "lines['GNIS_NAME']=flowlines.GNIS_NAME\n",
    "f = lambda x: x[:8]\n",
    "lines['HUC_8'] = lines.REACHCODE.map(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge selected fields from the associated tables into the clipped lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = lines.merge(VAA_tab, how='inner', left_on='COMID', right_on='ComID')\n",
    "lines = lines.merge(slope_tab, how='inner', left_on='COMID', right_on='COMID')\n",
    "lines = lines.merge(fcode_tab, how='inner', left_on='FCODE', right_on='FCode')\n",
    "\n",
    "lines['max_m'] = lines.MAXELEVSMO / 100.\n",
    "lines['min_m'] = lines.MINELEVSMO / 100.\n",
    "lines['StreamOrde'] = lines.StreamOrde\n",
    "lines['ArbolateSu'] = lines.ArbolateSu\n",
    "\n",
    "lines.drop(['FCode', 'ComID'], axis=1, inplace=True)\n",
    "# lines = lines[lines.Hydrograph != 'Intermittent']\n",
    "lines['intermit'] = lines.Hydrograph == 'Intermittent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a GeoDataFrame from the lines and get rid of lines outside the box (model) area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = gp.GeoDataFrame(lines, crs=prj4, geometry=lines.geometry)\n",
    "lines = lines[-lines.geometry.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a HUC-8 is provided in domain_diss, do the above operations again, this time for the basin outline rather than the model box.  It's faster to perform the operations in two steps like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prj_clip = domain_diss.geometry[0]\n",
    "lines_clip = lines['geometry'].intersection(prj_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_clip = gp.GeoDataFrame(lines_clip, crs=prj4, geometry=lines_clip.geometry)\n",
    "lines_clip['COMID'] = lines.COMID\n",
    "lines_clip['FCODE'] = lines.FCODE\n",
    "lines_clip['REACHCODE'] = lines.REACHCODE\n",
    "lines_clip['HUC_8'] = lines.HUC_8\n",
    "lines_clip['GNIS_NAME'] = lines.GNIS_NAME\n",
    "lines_clip['max_m'] = lines.max_m\n",
    "lines_clip['min_m'] = lines.min_m\n",
    "lines_clip['StreamOrde'] = lines.StreamOrde\n",
    "lines_clip['ArbolateSu'] = lines.ArbolateSu\n",
    "lines_clip['LENGTHKM'] = lines.LENGTHKM\n",
    "lines_clip['intermit'] = np.int32(lines.intermit)\n",
    "lines_clip['Hydroseq']=lines.Hydroseq\n",
    "lines_clip['FromNode']=lines.FromNode\n",
    "lines_clip['ToNode']=lines.ToNode\n",
    "\n",
    "\n",
    "lines_clip.drop(0, axis=1, inplace=True)\n",
    "lines_clip = lines_clip[-lines_clip.geometry.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove duplicate streams\n",
    "#first add a column to track if it's a duplicate\n",
    "lines_clip['Duplicate']=0\n",
    "hydroList = []\n",
    "for row in lines_clip.itertuples():\n",
    "    thisIndex = row.Index\n",
    "    thisHydro = row.Hydroseq\n",
    "    if thisHydro in hydroList:\n",
    "        lines_clip.loc[lines_clip.index==thisIndex,\"Duplicate\"]=1\n",
    "    else:\n",
    "        hydroList.append(thisHydro)\n",
    "        \n",
    "#keep only the 0's\n",
    "lines_clip = lines_clip[lines_clip.Duplicate==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confirm that the stream elevations track downward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#order the lines by hydroseq\n",
    "lines_clip.sort_values(by=\"Hydroseq\",inplace=True, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop through the lines\n",
    "thisNode = -999\n",
    "thisElev = -999\n",
    "indexList = lines_clip.index.values\n",
    "thisIter = 0\n",
    "for thisIndex in indexList:\n",
    "    #first, check that the start and end elevations are appropriate\n",
    "    thisStart = lines_clip.loc[lines_clip.index==thisIndex,\"max_m\"].values[0]\n",
    "    thisEnd = lines_clip.loc[lines_clip.index==thisIndex,\"min_m\"].values[0]\n",
    "    thisEndNode = lines_clip.loc[lines_clip.index==thisIndex,\"ToNode\"].values[0]\n",
    "    thisStartNode = lines_clip.loc[lines_clip.index==thisIndex,\"FromNode\"].values[0]\n",
    "    #if the end is higher than the start, then set the end equal to the start\n",
    "    if thisEnd > thisStart:\n",
    "        thisEnd = thisStart\n",
    "        lines_clip.loc[lines_clip.index==thisIndex,\"min_m\"] = thisEnd\n",
    "        #print(str(thisIter)+\": \" +str(thisIndex)+\": \" + str(lines_clip.loc[lines_clip.index==thisIndex,\"COMID\"].values[0]))\n",
    "    \n",
    "    #ensure that the elevation of the ToNode is consistent on all reaches that start at that node\n",
    "    lines_clip.loc[lines_clip.FromNode==thisEndNode,\"max_m\"]=thisEnd\n",
    "\n",
    "    thisIter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clipped flowlines to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_clip.to_file(os.path.join(model_ws, 'NHD_clip.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate values (offset and rotation) to transform flowlines to model grid reference system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xoff = origin[0]\n",
    "y_len_grid = NROW * LY\n",
    "yoff = origin[1] - y_len_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate and translate flowlines and coastal boundary to model grid reference system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#join the lines_clip to the coastal boundary shapefile to determine which segments are coastal segments\n",
    "\n",
    "coastalBound = gp.read_file(os.path.join(os.getcwd(),\"..\",\"input_data\",\"CoastalBoundary\",ms[\"saltwater_src\"]))\n",
    "coastalBound.to_crs(crs=lines_clip.crs, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not \"CoastalCT\" in md:\n",
    "    prj_clip = domain_diss.geometry[0]\n",
    "    coastalBound_clip = coastalBound['geometry'].intersection(prj_clip)\n",
    "else:\n",
    "    coastalBound_clip=coastalBound\n",
    "\n",
    "coastalBound_all = gp.geoseries.GeoSeries([geom for geom in coastalBound_clip.unary_union.geoms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_rot = lines_clip.rotate(-theta, origin=tuple(origin), use_radians=True).translate(xoff=-xoff, yoff=-yoff)\n",
    "coastalBound_rot = coastalBound_all.rotate(-theta, origin=tuple(origin), use_radians=True).translate(xoff=-xoff, yoff=-yoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create drain/river package input dictionary of lists (for FloPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First create a ModflowGrid object.  This class is available in FloPy and makes doing the intersections of flowlines with model grid easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NLAY = 1\n",
    "mfg = modgrid.ModflowGrid(NLAY, NROW, NCOL, LX, LY, 0)\n",
    "\n",
    "NPER =  1 \n",
    "drn_dict = {}\n",
    "perlist = []\n",
    "lineset = range(lines.shape[0])\n",
    "riv_loc = np.zeros((NROW,NCOL), np.int)\n",
    "riv_stg = np.zeros((NROW,NCOL), np.float)\n",
    "order_dict = {}\n",
    "per_order_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loop through each stress period. Each set of drains/rivers is a value in a dictionary whose key is the stress period.\n",
    "* Two parallel lists are used: lines_clip (in geographic space) and lines.rot (in model space)\n",
    " * intersections are done in model space so that lengths of flowline in model cell can be calculated\n",
    " * attributes aren't merged into lines_rot, so attributes have to be taken from lines_clip\n",
    " * the simple rotation and translation seems to preserve the order in both lists, but this could be a potential issue\n",
    "* mfg.intersection returns\n",
    " * nodes : tuples of (row, column) that are intersected by a flowline reach\n",
    " * lengths : scalar lengths of flowline in each model cell corresponding to the above tuple\n",
    "* Each reach has a starting and ending elevation from NHDPlus. Interpolate these to the midpoint of each intersected segment in each cell. If starting and ending elevations are the same, return zero-gradient stream segments.\n",
    "* Loop through each set of nodes for each reach, adding each one to the list of drain/river cells for the current stress period. \n",
    "* Some reaches have (probably erroneously) have begining and ending stages = 0. These are ignored, but this could be an issue for coastal areas with discharge to estuaries (stage = 0).\n",
    "* Maintain a second list with information on reach ID, original length, and stream order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#intersect the coastal boundary with the model grid\n",
    "coastalCellList = []\n",
    "for rindex, bound in coastalBound_all.iteritems():\n",
    "    try:\n",
    "        data = np.array(coastalBound_rot.geometry[rindex].exterior.coords[:])\n",
    "        nodes, lengths = mfg.intersection(data, 'polygon')\n",
    "        coastalCellList.extend(nodes)\n",
    "        print(rindex)\n",
    "#         interpolate the begin and end segment elevation to the midpoint of the feature in each cell\n",
    "\n",
    "    except (NotImplementedError, IndexError):\n",
    "\n",
    "        pass\n",
    "coastalCellList=list(set(coastalCellList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for period in range(NPER):\n",
    "    for rindex, reach in lines_clip.iterrows():\n",
    "        isCoastal = 0\n",
    "        try:\n",
    "            data = np.array(lines_rot.geometry[rindex].coords[:])\n",
    "            nodes, lengths = mfg.intersection(data, 'line')\n",
    "    #         interpolate the begin and end segment elevation to the midpoint of the feature in each cell\n",
    "            cumlen = np.cumsum(lengths)\n",
    "            midpoint = cumlen - np.asarray(lengths) / 2\n",
    "            rise = reach.max_m - reach.min_m\n",
    "            grad = rise / cumlen[-1]\n",
    "            stage = reach.max_m - midpoint * grad\n",
    "            if reach.max_m == reach.min_m:\n",
    "                stage = np.ones_like(stage) * reach.min_m\n",
    "    #         loop through the cells intersected by the feature\n",
    "            for nindex, (row, col) in enumerate(nodes):\n",
    "                if (tuple([row,col]) in coastalCellList) & (isCoastal==0):\n",
    "                    isCoastal = 1\n",
    "                riv_loc[row, col] = 1\n",
    "                temp = [0, row, col, stage[nindex], lengths[nindex]]\n",
    "                temp2 = [0, row, col, reach.StreamOrde, reach.ArbolateSu,reach.REACHCODE, reach.LENGTHKM, reach.intermit, isCoastal]        \n",
    "                #changing this to allow 0 stage segments\n",
    "#                 if stage[nindex] != 0:\n",
    "                perlist.append(temp)\n",
    "                per_order_list.append(temp2)\n",
    "        except (NotImplementedError, IndexError):\n",
    "            pass\n",
    "    drn_dict[period] = perlist\n",
    "    order_dict[period] = per_order_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the drain/river list and put those values in an array using (row, column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# riv_data = drn_dict\n",
    "riv_len = np.ones((NROW, NCOL), np.float) * hnoflo\n",
    "riv_stg = np.ones((NROW, NCOL), np.float) * hnoflo\n",
    "\n",
    "riv_data = drn_dict[0]\n",
    "\n",
    "#remove duplicate rows\n",
    "riv_data = np.vstack({tuple(row) for row in riv_data})\n",
    "\n",
    "for [l, r, c, s, cond] in riv_data:\n",
    "    row = int(r)\n",
    "    col = int(c)\n",
    "    stg = float(s)\n",
    "    stk = float(cond)\n",
    "    \n",
    "    #if there are multiple reaches within the cell, save the cumulative length and the lowest stage\n",
    "    if riv_len[row,col]==hnoflo:\n",
    "        riv_len[row, col] = stk\n",
    "    else:\n",
    "        riv_len[row, col] = stk + riv_len[row,col]\n",
    "    if riv_stg[row,col]==hnoflo:\n",
    "        riv_stg[row, col] = stg\n",
    "    else:\n",
    "        riv_stg[row, col] = np.min([stg,riv_stg[row, col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above but with the lists of stream order and reach ID. lengthkm is the original reach length from NHDPlus, not the intersected length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riv_data = order_dict\n",
    "riv_ord = np.ones((NROW ,NCOL), np.int) * hnoflo\n",
    "riv_comid = np.zeros((NROW, NCOL)).astype(str) \n",
    "riv_reachlen = np.ones((NROW, NCOL), np.int) * hnoflo\n",
    "riv_intermit = np.zeros((NROW, NCOL)).astype(bool)\n",
    "riv_ArbolateSum = np.ones((NROW ,NCOL), np.int) * hnoflo\n",
    "riv_coast = np.ones((NROW,NCOL),np.int)*hnoflo\n",
    "\n",
    "\n",
    "riv_data = order_dict[0]\n",
    "for [l, r, c, order, arbolateSum,comid, lengthkm, intermit,coast] in riv_data:\n",
    "    row = int(r) \n",
    "    col = int(c) \n",
    "    order = int(order)\n",
    "    reachlen = float(lengthkm)\n",
    "    arboSu = float(arbolateSum)\n",
    "    coast = int(coast)\n",
    "    \n",
    "    #if there are multiple reachs in the cell, save the highest order, the largest \n",
    "    #arbolate sum, the longest reach length, and coastal designation\n",
    "    if riv_ord[row,col]==hnoflo:\n",
    "        riv_ord[row, col] = order\n",
    "    else:\n",
    "        riv_ord[row, col] = np.max([order,riv_ord[row, col]])\n",
    "    if riv_ArbolateSum[row,col]==hnoflo:\n",
    "        riv_ArbolateSum[row, col] = arboSu\n",
    "    else:\n",
    "        riv_ArbolateSum[row, col] = np.max([arboSu,riv_ArbolateSum[row, col]])\n",
    "    riv_comid[row, col] = comid\n",
    "    if riv_reachlen[row,col]==hnoflo:\n",
    "        riv_reachlen[row, col] = reachlen\n",
    "    else:\n",
    "        riv_reachlen[row, col] = np.max([reachlen,riv_reachlen[row, col]])\n",
    "    if riv_coast[row,col]!=1:\n",
    "        riv_coast[row,col] = coast\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    riv_intermit[row, col] = intermit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an attribute to the lake file for rasterizing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_lakes = gp.read_file(lake_file)\n",
    "lakes = gp.read_file(lake_file)\n",
    "lakes['is_lake'] = 1\n",
    "lakes.to_crs(prj4, inplace=True)\n",
    "lakes.to_file(os.path.join(model_ws, 'lakes.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete the river data to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filesToDelete = [flowlines,lines,lines_clip,lines_rot,coastalBound,coastalBound_rot,coastalBound_all, coastalBound_clip]\n",
    "for thisThing in filesToDelete:\n",
    "    del thisThing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in the road network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read roadways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roads_clipped_file = os.path.join(model_ws, 'Road_clip.shp')\n",
    "if os.path.exists(roads_clipped_file):\n",
    "    roads_clip = gp.read_file(roads_clipped_file)\n",
    "else:\n",
    "    road_file = r\"C:/workspace/gw-general-models/Genmod1.0/input_data/Transportation/Roads.shp\"\n",
    "    roadLines = gp.read_file(road_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the roadways to the box and project it back to the projected coordinate system (probably Albers). Note: the crs from NHD is given below as epsg code 4269.  This is the same as a plain NAD83 crs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(roads_clipped_file):\n",
    "    roads = roadLines['geometry'].intersection(latlong_clip)\n",
    "    roads = roads.to_crs(crs=prj4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a GeoDataFrame from the roads and get rid of roads outside the box (model) area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(roads_clipped_file):\n",
    "    roads = gp.GeoDataFrame(roads, crs=prj4, geometry=roads.geometry)\n",
    "    roads = roads[-roads.geometry.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a HUC-8 is provided in domain_diss, do the above operations again, this time for the basin outline rather than the model box.  It's faster to perform the operations in two steps like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(roads_clipped_file):\n",
    "    prj_clip = domain_diss.geometry[0]\n",
    "    roads_clip = roads['geometry'].intersection(prj_clip)\n",
    "    del roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clipped roads to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(roads_clipped_file):\n",
    "    roads_clip = roads_clip[-roads_clip.geometry.isnull()]\n",
    "    roads_clip.to_file(roads_clipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roads_clip = gp.GeoDataFrame(roads_clip, crs=prj4, geometry=roads_clip.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roads_rot = roads_clip.rotate(-theta, origin=tuple(origin), use_radians=True).translate(xoff=-xoff, yoff=-yoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "road_dict = {}\n",
    "perlist = []\n",
    "for period in range(NPER):\n",
    "    for rindex, road in roads_clip.iterrows():\n",
    "        try:\n",
    "            data = np.array(roads_rot.geometry[rindex].coords[:])\n",
    "            nodes, lengths = mfg.intersection(data, 'line')\n",
    "    #         loop through the cells intersected by the feature\n",
    "            for nindex, (row, col) in enumerate(nodes):\n",
    "                temp = [0, row, col,lengths[nindex]]\n",
    "                perlist.append(temp)\n",
    "        except (NotImplementedError, IndexError):\n",
    "            pass\n",
    "    road_dict[period] = perlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the road list and put those values in an array using (row, column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "road_Arr = np.zeros((NROW, NCOL),np.int8)\n",
    "road_Len = np.zeros((NROW,NCOL),np.float)\n",
    "\n",
    "road_data = road_dict[0]\n",
    "\n",
    "#remove duplicate rows\n",
    "road_data = np.vstack({tuple(row) for row in road_data})\n",
    "\n",
    "for [l, r, c,roadLen] in road_data:\n",
    "    row = int(r)\n",
    "    col = int(c)\n",
    "    \n",
    "    road_Arr[row, col] = 1\n",
    "    \n",
    "    #if there are multiple roads within the cell, save the cumulative length\n",
    "    if road_Len[row,col]==0:\n",
    "        road_Len[row, col] = roadLen\n",
    "    else:\n",
    "        road_Len[row, col] = roadLen + road_Len[row,col]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filesToDelete = [roads_clip, roads_rot]\n",
    "for thisThing in filesToDelete:\n",
    "    del thisThing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project data sources onto modflow grid, store the data in a file called model_grid.csv, and write Geotiff images of all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a blank dataframe and add 2D node numbers (NROW * NCOL) as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid = pd.DataFrame()\n",
    "model_grid['node_num'] = np.arange(NROW * NCOL)\n",
    "model_grid.set_index('node_num', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and read the NED grid for the current model area and store its projection. All other data sources will be projected into this projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "ned = gdal.Open(src)\n",
    "shapeproj = ned.GetProjection()\n",
    "ned = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add vector data sources to model_grid dataframe.\n",
    "* Better: put parameters in a nested list and loop through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is from an inset or other more detailed model, read the ibound array based on the corresponding General Model for consistency. This is coded in the second block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if 'ibound_src' in ms.keys():\n",
    "#     ib_src = model_dict[ms['ibound_src']]['ws']\n",
    "#     ib_src = os.path.join(proj_dir, ib_src)\n",
    "#     ib_file = model_dict[ms['ibound_src']]['df']\n",
    "#     ib_shp = os.path.join(ib_src, ib_file)\n",
    "#     grid = process_vector_data(ib_shp, 'ibound')\n",
    "#     model_grid['ibound'] = grid.ravel()   \n",
    "# else:\n",
    "src = os.path.join(model_ws, 'domain_outline.shp')\n",
    "grid = process_vector_data(src, 'ibound')\n",
    "model_grid['ibound'] = grid.ravel()\n",
    "\n",
    "#src = os.path.join(model_ws, gage_file)\n",
    "#grid = process_vector_data(src, 'GAGE_ID')\n",
    "#model_grid['gage_id'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(os.getcwd(),\"..\",\"..\",\"..\",\"SHEDS\",\"spatial_01\",\"spatial_01\",\"Catchments01.shp\")\n",
    "grid = process_vector_data(src, 'LocalID')\n",
    "model_grid['SHEDS'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(model_ws, 'NHD_clip.shp')\n",
    "grid = process_vector_data(src, 'ArbolateSu')\n",
    "model_grid['arbolateSu'] = grid.ravel()\n",
    "\n",
    "gess_geology_file = 'GESS_poly.gdb'\n",
    "src = os.path.join(qa_dir, gess_geology_file)\n",
    "grid = process_vector_data(src, 'CrseStratSed')\n",
    "model_grid['gess_poly'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(model_ws, 'lakes.shp')\n",
    "grid = process_vector_data(src, 'is_lake')\n",
    "model_grid['lake'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"SurficialGeology\",\"SurficialThickness.shp\")\n",
    "grid = process_vector_data(src,'Thickness')\n",
    "#first do an interpolation using griddata, and then if any remain, use the nearest neighbor\n",
    "grid = interp_grid(grid, naVal = 0)\n",
    "grid = fill(grid, grid==1.0)\n",
    "model_grid['kauffman_CTquat_thk'] = grid.ravel()\n",
    "\n",
    "\n",
    "#using a unique ID other than HUC12 codes b/c HUC12 codes are too long for rasterizing (causing weird errors)\n",
    "src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"NHDPlusV2Data\",\"NHDPlusNE_MA\",\"NHDPlus01_02\",\"WBDSnapshot\",\"WBD\",\"WBD_Subwatershed_CoastalCT.shp\")\n",
    "grid = process_vector_data(src, 'Zone')\n",
    "model_grid['HUC12_shortCD'] = grid.ravel()\n",
    "\n",
    "#then extend the HUC12 to the model boundary\n",
    "#first do an interpolation using griddata, and then if any remain, use the nearest neighbor\n",
    "grid[grid==4613]=0.0\n",
    "grid[grid==4204]=0.0\n",
    "grid[grid==4205]=0.0\n",
    "grid[grid==4345]=0.0\n",
    "grid = fill(grid, grid==0.0)\n",
    "model_grid['HUC12_shortCD_ext'] = grid.ravel()*model_grid['ibound']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if 'sewer_src' in ms.keys():\n",
    "    sewer_src = ms['sewer_src']\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"WaterUse\",sewer_src)\n",
    "    grid = process_vector_data(src,'Sewer')\n",
    "    model_grid['sewer'] = grid.ravel()\n",
    "    \n",
    "if 'pws_src' in ms.keys():\n",
    "    pws_src = ms['pws_src']\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"WaterUse\",pws_src)\n",
    "    grid = process_vector_data(src,'PWS')\n",
    "    model_grid['pws'] = grid.ravel()\n",
    "    \n",
    "if 'well_src' in ms.keys():\n",
    "    well_src = ms['well_src']\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"WaterUse\",well_src)\n",
    "    #this option gets the sum of the fluxes for all wells (https://gis.stackexchange.com/questions/167757/how-to-rasterize-a-point-layer-with-multiple-features-falling-inside-a-raster-ce)\n",
    "    grid = process_vector_data(src,'Flux', options=['MERGE_ALG=ADD'])\n",
    "    model_grid['well_PWSFlux'] = grid.ravel()\n",
    "    \n",
    "    grid = process_vector_data(src,'Glacial')\n",
    "    model_grid['well_Glacial'] = grid.ravel()\n",
    "    \n",
    "if 'bedrock_geo' in ms.keys():\n",
    "    bedrock_src = ms['bedrock_geo']\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"BedrockGeology\",bedrock_src)\n",
    "    grid = process_vector_data(src,'LithSim2Cd')\n",
    "    grid = fill(grid, grid==0)\n",
    "    model_grid['BedLithSimp'] = grid.ravel()\n",
    "    \n",
    "if 'surf_geo' in ms.keys():\n",
    "    surficial_src = ms['surf_geo']\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"SurficialGeology\",surficial_src)\n",
    "    grid = process_vector_data(src,'SurfCode')\n",
    "    grid = fill(grid, grid==0)\n",
    "    model_grid['SurfGeo'] = grid.ravel()\n",
    "    \n",
    "if 'zone_src' in ms.keys():\n",
    "    zone_src = ms['zone_src']\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"Zones\",zone_src)\n",
    "    grid = process_vector_data(src,'Zone', options=['ALL_TOUCHED=True'])\n",
    "    model_grid['embayment'] = grid.ravel()*model_grid['ibound']\n",
    "    \n",
    "    \n",
    "if 'NHD_catchment_src' in ms.keys():\n",
    "    if ms['NHD_catchment_src']:\n",
    "        src = os.path.join(os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusCatchment', 'Catchment.shp'))\n",
    "        grid = process_vector_data(src,'FEATUREID')\n",
    "        model_grid['NHD_catchment'] = grid.ravel()*model_grid['ibound']\n",
    "if 'NHD_HRU' in ms.keys():\n",
    "    src = os.path.join(ms['NHD_HRU'])\n",
    "    grid = process_vector_data(src,'hru_id_nat')\n",
    "    model_grid['NHD_hru'] = grid.ravel()*model_grid['ibound']\n",
    "    \n",
    "try:\n",
    "    src = r'C:\\Workspace\\gw-general-models\\Genmod1.0\\input_data\\Gages\\CoastalCTbaseflowGages.shp'\n",
    "    grid = process_vector_data(src,'localID')\n",
    "    model_grid['baseflowGage'] = grid.ravel()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add raster data sources to model_grid dataframe.\n",
    "* Better: put parameters in a nested list and loop through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the pws to remove isolated cells with private wells (likely in error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 0.01)\n",
    "model_grid['ned'] = grid.ravel() # units of meters\n",
    "\n",
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'elev_cm')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 0.01)\n",
    "model_grid['ned_mean'] = grid.ravel() # units of meters\n",
    "\n",
    "model_grid['kauffman_bedrock_el'] = model_grid['ned']-model_grid['kauffman_CTquat_thk']\n",
    "\n",
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NEDSnapshot', rpu, 'slope_deg')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 1)\n",
    "model_grid['slope'] = grid.ravel() # units of degrees\n",
    "\n",
    "\n",
    "src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"CoastalBoundary\",\"CoastalBoundary_ned.tif\")\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 0.01)\n",
    "model_grid['ned_coast_min'] = grid.ravel() # units of meters\n",
    "\n",
    "#require the ned_coast_min to be less than (or equal to) the ned_mean\n",
    "model_grid.loc[model_grid.ned_coast_min>model_grid.ned_mean,\"ned_coast_min\"]=model_grid.loc[model_grid.ned_coast_min>model_grid.ned_mean,\"ned_mean\"]\n",
    "\n",
    "src = os.path.join(nhd_dir, nhd_basin_dir, 'NHDPlusCatchment', 'cat')\n",
    "grid = process_raster_data(src, gdal.GRA_NearestNeighbour, 1)\n",
    "model_grid['catchment'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(soller_surf_thick_dir, 'sim3392_sheet1_driftthickness.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, ft2m) # convert feet to meters\n",
    "model_grid['soller_thk'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(soller_bedrock_topo_dir, 'sim3392_sheet2_bedrocktopo.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, ft2m) # convert feet to meters\n",
    "model_grid['soller_bedrock_el'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(nlcd_dir, 'nlcd_2011_landcover_2011_edition_2014_10_10.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Mode, 1)\n",
    "model_grid['nlcd'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(clear_dir, 'LIS_landcover2010_v23_ctsp83.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Mode, 1)\n",
    "model_grid['clear'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(clear_dir, 'LIS_grass.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 1)\n",
    "model_grid['clear_per_grass'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(nlcd_dir, 'NLCD_agLand.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 1)\n",
    "model_grid['nlcd_ag'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(nlcd_dir, 'wetland.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 1)\n",
    "model_grid['WetlandPer'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(nlcd_dir, 'High_Medium_Intensity_Developed.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 1)\n",
    "model_grid['PerDev'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(nlcd_dir,r\"..\\NLCD_2011_Impervious_L48_20190405\", 'NLCD_2011_Impervious_L48_20190405.img')\n",
    "grid = process_raster_data(src, gdal.GRA_Average, 1)\n",
    "model_grid['PerImp'] = grid.ravel()\n",
    "\n",
    "\n",
    "#population density\n",
    "src = os.path.join(pop_den_dir,'pden2010_60m.tif')\n",
    "grid = process_raster_data(src,gdal.GRA_Average,((L*L/1000/1000))) #converts from people / km2 to people / model cell\n",
    "model_grid['population']=grid.ravel()\n",
    "\n",
    "\n",
    "#Three sources for recharge values\n",
    "#SWB_NAWQA published values\n",
    "src = os.path.join(alt_recharge_SWB_NAWQA, 'SWB_Nawqa_mm.tif') #SWB_NAWQA, published, values in this file are mm/year, need to convert to m\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 0.001)\n",
    "model_grid['rch_eff_m_SWB_NAWQA'] = grid.ravel()\n",
    "\n",
    "\n",
    "#Reitz published values\n",
    "src = os.path.join(recharge_reitz, 'RC_eff_2013.tif') #Reitz, published, values in this file are m/year, no need to convert\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 1.0)\n",
    "model_grid['rch_eff_m_Reitz_2013'] = grid.ravel()\n",
    "\n",
    "#The Wolock recharge below is included as an alternative to Reitz for comparison purposes; user will have to specify in JN3\n",
    "src = os.path.join(alt_recharge_wolock, 'rech48grd') #Wolock, published, values in this file are mm/year, need to convert to m\n",
    "grid = process_raster_data(src, gdal.GRA_Bilinear, 0.001)\n",
    "model_grid['rch_m_Wolock'] = grid.ravel()\n",
    "\n",
    "if 'saltwater_src' in ms.keys():\n",
    "    \n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"Wetlands\",\"CoastalBoundary_Coastal_Deep.tif\")\n",
    "    grid = process_raster_data(src,gdal.GRA_Average, 1.0)\n",
    "    model_grid['Coast_Deep'] = grid.ravel()\n",
    "    #model_grid[model_grid.Coast_Deep>0]=1\n",
    "    \n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"Wetlands\",\"CoastalBoundary_Coastal_Shallow.tif\")\n",
    "    grid = process_raster_data(src,gdal.GRA_Average, 1.0)\n",
    "    model_grid['Coast_Shallow'] = grid.ravel()\n",
    "    #model_grid[model_grid.Coast_Shallow>0]=1\n",
    "\n",
    "if 'wetlands_src' in ms.keys():\n",
    "    src = os.path.join(os.getcwd(),\"..\",\"input_data\",\"Wetlands\",\"CoastalBoundary_Wetland.tif\")\n",
    "    grid = process_raster_data(src,gdal.GRA_Average, 1.0)\n",
    "    model_grid['PerWater'] = grid.ravel()\n",
    "    \n",
    "if 'soil_src' in ms.keys():\n",
    "    src = os.path.join(soil_dir,\"PerSand_60to200cm.tif\")\n",
    "    grid = process_raster_data(src,gdal.GRA_Average, 1.0)\n",
    "    model_grid['PerSand'] = grid.ravel()\n",
    "    \n",
    "    src = os.path.join(soil_dir,\"PerClay_60to200cm.tif\")\n",
    "    grid = process_raster_data(src,gdal.GRA_Average, 1.0)\n",
    "    model_grid['PerClay'] = grid.ravel()\n",
    "    \n",
    "    src = os.path.join(soil_dir,\"PerSilt_60to200cm.tif\")\n",
    "    grid = process_raster_data(src,gdal.GRA_Average, 1.0)\n",
    "    model_grid['PerSilt'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(NDep_dir,\"n_dw-20060\")\n",
    "grid = process_raster_data(src,gdal.GRA_Average, ((L*L/10000))) #converts from kg-N / ha / yr to kg-N / model cell / yr\n",
    "model_grid['NDep_dry'] = grid.ravel()\n",
    "\n",
    "src = os.path.join(NDep_dir,\"n_ww-20060\")\n",
    "grid = process_raster_data(src,gdal.GRA_Average, ((L*L/10000))) #converts from kg-N / ha / yr to kg-N / model cell / yr\n",
    "model_grid['NDep_wet'] = grid.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process MOHP data. Each region could have a different number of stream-order-based MOHP data sources, so this processing has to be handled differently from regular raster sources.  One complication is that the projection information has to be translated from ESRI's custom version of the WKT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:   \n",
    "    srcdir = mohp_dir\n",
    "    rpu_file = 'pctdistR{}'.format(int(rpu[3:5]))\n",
    "    tif_files = []\n",
    "\n",
    "# search MOHP first-generation directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for f in files:\n",
    "            if f.endswith('tif'):\n",
    "                if rpu_file in f:\n",
    "                    src_pth = os.path.join(root, f)\n",
    "                    tif_files.append(src_pth)\n",
    "                \n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'HP_{}'.format(key[9:])\n",
    "        model_grid[key] = arr[:, col]\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:  \n",
    "    srcdir = mohp2_dir\n",
    "    tif_files = []\n",
    "\n",
    "# search MOHP second-generation directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for f in files:\n",
    "            if f.endswith('tif'):\n",
    "                src_pth = os.path.join(root, f)\n",
    "                tif_files.append(src_pth)\n",
    "\n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'HP2_{}'.format(key.split('_')[0])\n",
    "        model_grid[key] = arr[:, col]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    srcdir = thies2_dir\n",
    "    tif_files = []\n",
    "\n",
    "# search MOHP-Thiessen polygon directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for f in files:\n",
    "            if f.endswith('tif'):\n",
    "                src_pth = os.path.join(root, f)\n",
    "                tif_files.append(src_pth)\n",
    "\n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'HPTH_{}'.format(key.split('_')[0])\n",
    "        model_grid[key] = arr[:, col]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    srcdir = pour_dir\n",
    "    rpu_file = 'or{}'.format(int(rpu[3:5]))\n",
    "    grid_files = []\n",
    "\n",
    "# search MOHP pour-point-elevation directory for tif files and store them in a list\n",
    "    for root, dirs, files in os.walk(srcdir):\n",
    "        for d in dirs:\n",
    "            if d[:2] == 'or':\n",
    "                src_pth = os.path.join(root, d)\n",
    "                grid_files.append(src_pth)\n",
    "\n",
    "# process MOHP data into an array corresponding to the model grid\n",
    "    arr = process_mohp_data(tif_files)\n",
    "\n",
    "# unpack the array and add each column to model_grid\n",
    "    for col, src in enumerate(tif_files):\n",
    "        key = os.path.basename(src).split('.')[0]\n",
    "        key = 'PP_{}'.format(key[2:])\n",
    "        model_grid[key] = arr[:, col]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the ibound array to get rid of disconnected cells.  This is a trial-and-error process. The filter is selected in model_spec.py. One of these filters can be used if there are isolated nodes appearing because of the discretization. The filter is specified in the model dictionary in model_spec.py.\n",
    "\n",
    "use nd.image.measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ib = model_grid.ibound.reshape(NROW, NCOL)\n",
    "# if ib_filter == 0:\n",
    "#     pass\n",
    "# elif ib_filter == 1:\n",
    "#     filtr = np.array([[0,1,0], [1,4,1], [0,1,0]])\n",
    "#     ib = nd.correlate(ib, filtr, mode='constant', cval=0) > 5\n",
    "# elif ib_filter == 2:\n",
    "#     # alternate filter 11/5/2014\n",
    "#     filtr = np.array([[0,0,1,0,0], [0,0,4,0,0], [1,4,16,4,1], [0,0,4,0,0], [0,0,1,0,0]])\n",
    "#     ib = nd.correlate(ib, filtr, mode='constant', cval=0) > 29\n",
    "# else: \n",
    "#     pass\n",
    "\n",
    "# ib = ib.astype(int)\n",
    "# model_grid['ibound'] = ib.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An improved filter than that above.\n",
    "ib = model_grid.ibound.values.reshape(NROW, NCOL)\n",
    "'''\n",
    "Removes isolated active cells from the IBOUND array.\n",
    "Args: ibound array (nlay,nrow,ncol)\n",
    "\n",
    "from Wesley Zell\n",
    "'''\n",
    "\n",
    "# Distinguish disconnected clusters of active cells in the IBOUND array.\n",
    "# 0 is considered background; in MODLFOW, active cells are != 0.\n",
    "ib[ib != 0] = 1\n",
    "array_of_cluster_idx, num = nd.measurements.label(ib)\n",
    "\n",
    "# Identify the cluster with the most active cells; this is the main active area\n",
    "areas = nd.measurements.sum(ib, array_of_cluster_idx,\\\n",
    "                         index=np.arange(array_of_cluster_idx.max() + 1))\n",
    "max_cluster_idx = np.argmax(areas)\n",
    "\n",
    "# Inactivate all cells that belong to secondary clusters (e.g., islands)\n",
    "ib[array_of_cluster_idx != max_cluster_idx] = 0\n",
    "ib = ib.astype(int)\n",
    "model_grid['ibound'] = ib.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter ibound to make a grid of edge cells. Intersect this with the lake grid. Cell that are on the edge of the model and are in lakes could be treated as GHB cells.  This is not implemented yet, but the code would be very similar to the code above for drain/river cells. FloPy GHB requires a dictionary of lists similar to river/drain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtr = np.ones((3,3))\n",
    "not_solid = nd.correlate(ib, filtr, mode='constant', cval=0) < 9\n",
    "\n",
    "edge = (ib + not_solid) == 2\n",
    "edge = edge.astype(int)\n",
    "model_grid['edge'] = edge.ravel()\n",
    "\n",
    "lakes_arr = model_grid.lake.values.reshape(NROW, NCOL)\n",
    "lake_blur = nd.correlate(lakes_arr, filtr, mode='constant', cval=0) > 0\n",
    "\n",
    "ghb = (edge + lake_blur) == 2\n",
    "ghb = ghb.astype(int)\n",
    "model_grid['ghb'] = ghb.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add drain/river information to model_grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_grid['stage'] = deepcopy(riv_stg.ravel())\n",
    "model_grid['segment_len'] = riv_len.ravel()\n",
    "model_grid['order'] = riv_ord.ravel()\n",
    "model_grid['arbolateSum'] = riv_ArbolateSum.ravel()\n",
    "model_grid['reachcode'] = riv_comid.ravel()\n",
    "model_grid['reach_intermit'] = riv_intermit.ravel()\n",
    "model_grid['reach_len'] = riv_reachlen.ravel()\n",
    "model_grid['reach_int'] = pd.Categorical(model_grid.reachcode).codes\n",
    "model_grid['riv_coast'] = riv_coast.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the river stage at sea level where it is lower\n",
    "model_grid.loc[(model_grid.stage<sea_level)&(model_grid.stage>-99),\"stage\"]=sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the road information to model_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_grid['road']=road_Arr.ravel()\n",
    "model_grid['road_length']=road_Len.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NHDPlus has a small number of MAXELEVSMO and MINELEVSMO in (mostly?) first order streams that are set to a missing value code -9980. Converted to meters this is -99.80. Replace all such values with the NED value minus 1 (meter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid['stage'] = np.where(model_grid.stage == -99.98, model_grid.ned - 1., model_grid.stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GHB is used for marine boundaries along cells (and rivers) within the coastal boundary shapefile (marine or estuarine wetlands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_grid['ghb_sea']=0\n",
    "#setting the ghb for cells within deep or shallow marine / estuarine wetlands or coastal rivers\n",
    "#(some places where the rivers should be coastal but were missed )\n",
    "model_grid.loc[((model_grid['Coast_Deep']>0) | (model_grid['Coast_Shallow']>0)| (model_grid['riv_coast']==1))&(np.isfinite(model_grid.ned_coast_min)),'ghb_sea']=1\n",
    "\n",
    "#convert the depth of saltwater to a freshwater head using the depth and the densities of each\n",
    "#fresh_head = (den_sea / den_fresh)*sea_level - [(den_sea - den_fresh)/den_fresh]*depth_sea\n",
    "model_grid['fresh_head']=(den_salt / den_fresh)*sea_level + ((den_salt-den_fresh)/den_fresh)*(sea_level - model_grid['ned_coast_min'])\n",
    "\n",
    "#if the elevation is > sea level, assume salinity = 0\n",
    "model_grid.loc[model_grid['ned_coast_min']>sea_level,'fresh_head']= model_grid['ned_coast_min']\n",
    "\n",
    "#changing rivers within the coastal boundary to ghb using salinity = 0 and the river stage\n",
    "model_grid.loc[(model_grid['ghb_sea']==1) & (model_grid.stage> -9990),'fresh_head']=model_grid.loc[(model_grid['ghb_sea']==1) & (model_grid.stage> -9990),'stage']\n",
    "\n",
    "#some river stages are less than 0\n",
    "\n",
    "#change the stage to np.nan to avoid adding a drain as well\n",
    "model_grid.loc[(model_grid['ghb_sea']==1) & (model_grid.stage> -9990),'stage']=-9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a column noting the major areas of coastal waters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.04634385,  0.71807291,  5.82499981])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid['Coast_all']=model_grid.Coast_Deep + model_grid.Coast_Shallow\n",
    "model_grid['Coast_major']=0\n",
    "model_grid.loc[model_grid.Coast_all>0.85,\"Coast_major\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter the major coastal waters areas to remove isolated 'islands' of coastal waters\n",
    "cm = model_grid.Coast_major.values.reshape(NROW, NCOL)\n",
    "ibound =  model_grid.ibound.values.reshape(NROW,NCOL)\n",
    "cm = cm * ibound\n",
    "'''\n",
    "Removes isolated active cells from the IBOUND array.\n",
    "Args: ibound array (nlay,nrow,ncol)\n",
    "\n",
    "from Wesley Zell\n",
    "'''\n",
    "\n",
    "# Distinguish disconnected clusters of active cells in the Coast_major array.\n",
    "# 0 is considered background; in MODLFOW, active cells are != 0.\n",
    "array_of_cluster_idx, num = nd.measurements.label(cm)\n",
    "\n",
    "# Identify the clusters with > 500 cells, these are the major coastal areas\n",
    "areas = nd.measurements.sum(cm, array_of_cluster_idx,\\\n",
    "                         index=np.arange(array_of_cluster_idx.max() + 1))\n",
    "mainAreas = np.where(areas>500)\n",
    "\n",
    "# Inactivate all cells that belong to secondary clusters (e.g., islands)\n",
    "cm[cm!=0]=0\n",
    "for thisID in mainAreas[0]:\n",
    "    cm[array_of_cluster_idx ==thisID] = 1\n",
    "cm = cm.astype(int)\n",
    "model_grid['Coast_major'] = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set the river stage at sea level where it is below sea level\n",
    "model_grid['stage'] = np.where((model_grid.stage < sea_level) & (model_grid.stage>-999), sea_level, model_grid.stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#require ghb_sea boundaries > 0 to be at or below the elevation of the nearest river\n",
    "if not inlandCoastalAsDrains:\n",
    "    ghbGTsea_level = (model_grid.fresh_head>sea_level)&(model_grid.stage<-999)&(model_grid.ibound==1)  #get the cells with ghb>sea level\n",
    "else:\n",
    "    ghbGTsea_level = (model_grid.fresh_head>sea_level)&(model_grid.stage<-999)&(model_grid.ibound==1)&(model_grid.Coast_major==1)  #get the cells with ghb>sea level\n",
    "\n",
    "ghbGTsea_levelArr = ghbGTsea_level.values.reshape(NROW,NCOL)\n",
    "ghbPrior = model_grid.fresh_head.values.reshape(NROW,NCOL)\n",
    "ghbUpdated = np.ones((NROW, NCOL))*-9999\n",
    "x, y = np.mgrid[0:NROW, 0:NCOL]\n",
    "tree = ss.KDTree(list(zip(x.ravel(), y.ravel())))\n",
    "outPut = tree.query(np.transpose(np.nonzero(ghbGTsea_levelArr)),k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.04634385,  0.71807291,  5.82499981])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#loop through the high ghb cells\n",
    "hucArr = model_grid.HUC12_shortCD_ext.values.reshape(NROW,NCOL)\n",
    "for i in range(len(outPut[1])):\n",
    "    x,y=np.split(tree.data[outPut[1][i]],2,axis=1)\n",
    "    #get the huc for this cell\n",
    "    thisHUC = hucArr[x[0],y[0]][0]\n",
    "    tempStg = deepcopy(riv_stg)\n",
    "    tempStg[(tempStg>-9000)&(tempStg<0)]=0\n",
    "    tempStg[hucArr!=thisHUC]=-9999\n",
    "    stgList = tempStg[x.flatten(),y.flatten()]\n",
    "    if np.all(stgList==-9999):\n",
    "        ghbUpdated[x[0],y[0]] =0\n",
    "    else:\n",
    "        nearestRiv = np.min(np.where(stgList>-999))\n",
    "        ghbUpdated[x[0],y[0]]=min(stgList[nearestRiv],ghbPrior[x[0],y[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.04634385,  0.71807291,  5.82499981])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#add the updated ghb levels back into the model_grid data frame\n",
    "ghbUpdated2 = ghbUpdated.ravel()\n",
    "model_grid.loc[ghbUpdated2>-9999,\"fresh_head\"] = ghbUpdated2[ghbUpdated2>-9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#require ghb boundaries within the huc12 boundaries to be >=sealevel\n",
    "model_grid.loc[(model_grid.fresh_head<sea_level)&(model_grid.HUC12_shortCD!=0),\"fresh_head\"]=sea_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace hnoflo values with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid[model_grid == hnoflo] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace grid-cell-mean NED elevations with interpolated stream stage, but only in model cells that contain a stream. The resulting data is called 'top'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid['top'] = model_grid.ned_mean\n",
    "index = model_grid.stage.notnull()\n",
    "model_grid.loc[index, 'top'] = model_grid.stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace grid-cell mean NED elevations with fresh head for ghb cells with top above fresh head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = (model_grid.ghb_sea==1) & (model_grid.fresh_head<model_grid.top)\n",
    "model_grid.loc[index, 'top'] = model_grid.fresh_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not inlandCoastalAsDrains:\n",
    "#reduce the elevation of ghb boundaries > 0 by 3/4 to prevent excessive inflow\n",
    "    model_grid.loc[(model_grid.fresh_head>sea_level),\"fresh_head\"]=0.25*model_grid.loc[(model_grid.fresh_head>sea_level),\"fresh_head\"]\n",
    "else:\n",
    "    model_grid.loc[(model_grid.fresh_head>sea_level)&(model_grid.Coast_major==1),\"fresh_head\"]=0.25*model_grid.loc[(model_grid.fresh_head>sea_level)&(model_grid.Coast_major==1),\"fresh_head\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.04634385,  0.71807291,  5.82499981])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add layer, row, column to model_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid['lay'] = 0\n",
    "row, col = np.mgrid[0:NROW:1, 0:NCOL:1]\n",
    "\n",
    "model_grid['row'] = row.ravel()\n",
    "model_grid['col'] = col.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct the top elevations for cells with drains / ghb to ensure they slope downward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid.reset_index(inplace=True, drop=False)\n",
    "model_grid.set_index(inplace=True,drop=False,keys=\"node_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the list of nodes to check\n",
    "elevationsToCheck = (model_grid.ibound==1)&((model_grid.fresh_head>0) | (np.isfinite(model_grid.stage)) | (model_grid.PerWater>0))\n",
    "elevData = model_grid.loc[elevationsToCheck,[\"node_num\",\"top\"]]\n",
    "#sort through the cells from highest to lowest\n",
    "elevData.sort_values(\"top\",ascending=False,inplace=True)\n",
    "nodeList = [x for x in elevData.node_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n",
      "0\n",
      "[ 0.          0.          0.04634385  0.71807291  5.82499981]\n"
     ]
    }
   ],
   "source": [
    "def checkNeighbors(thisNode):\n",
    "    thisRow = model_grid.loc[model_grid.node_num==thisNode,\"row\"].values[0]\n",
    "    thisCol = model_grid.loc[model_grid.node_num==thisNode,\"col\"].values[0]\n",
    "    thisTop = model_grid.loc[model_grid.node_num==thisNode,\"top\"].values[0]\n",
    "    #neighbors only include the 4 adjacent cells (not the corners)\n",
    "    neighbors = model_grid.loc[((model_grid.row==thisRow)|(model_grid.col==thisCol))&(model_grid.row>=thisRow-1)&(model_grid.row<=thisRow+1)&(model_grid.col>=thisCol-1)&(model_grid.col<=thisCol+1),['ibound','top','row','col']]\n",
    "    \n",
    "    #neighbors also includes the cell of interest\n",
    "    if np.any(neighbors.ibound==0):\n",
    "    #     continue\n",
    "        newMin = -9999\n",
    "        newNodes = []\n",
    "    else:\n",
    "        if neighbors[neighbors.top<=thisTop].shape[0]>1:   #this is true when 1 of the neighbors is below the cell of interest\n",
    "            #and false when the cell of interest is the lowest cell\n",
    "    #         continue\n",
    "            newMin = -9999\n",
    "            newNodes = []\n",
    "        else:\n",
    "            #get the minimum neighbor (the lowest land surface / sea-floor altitude of the neighboring cells)\n",
    "            newMin = np.min(neighbors.top[neighbors.top!=thisTop])\n",
    "                \n",
    "#             print(\"Row: %s, Col: %s\"%(str(thisRow),str(thisCol)))\n",
    "#             print(\"New Min: %s, Old Min: %s\"%(str(newMin),str(thisTop)))\n",
    "#             print(thisNode)\n",
    "            newNodes = [x for x in neighbors.index.values]\n",
    "            #update the elevation\n",
    "            model_grid.loc[model_grid.node_num==thisNode,\"top\"]=newMin\n",
    "            \n",
    "            #if the newMin > sea_level, update the fresh_head or stage, otherwise don't\n",
    "            if newMin>sea_level:\n",
    "                if model_grid.loc[model_grid.node_num==thisNode,\"fresh_head\"].values[0]>0:\n",
    "                    model_grid.loc[model_grid.node_num==thisNode,\"fresh_head\"]=newMin\n",
    "                elif np.isfinite(model_grid.loc[model_grid.node_num==thisNode,\"stage\"].values[0]):\n",
    "                    model_grid.loc[model_grid.node_num==thisNode,\"stage\"]=newMin\n",
    "    \n",
    "    return newMin,newNodes\n",
    "\n",
    "print(len(nodeList))\n",
    "\n",
    "for thisNode in nodeList:\n",
    "    if np.where(nodeList==thisNode)[0][0]%10000==0:\n",
    "        print(np.where(nodeList==thisNode)[0][0])\n",
    "        print(np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100]))\n",
    "    # thisNode = 63181    \n",
    "    nodesToCheck = True\n",
    "    newMin,newNodes = checkNeighbors(thisNode)\n",
    "    if len(newNodes)==0:\n",
    "        nodesToCheck = False\n",
    "    nodeList2 = newNodes\n",
    "    nodeList2 = [x for x in nodeList2 if x!= thisNode]\n",
    "    while nodesToCheck:\n",
    "        thisNode = nodeList2[0]\n",
    "        newMin,newNodes = checkNeighbors(thisNode)\n",
    "        nodeList2.extend(newNodes)\n",
    "        nodeList2 = np.unique(nodeList2)\n",
    "        nodeList2 = [x for x in nodeList2 if x != thisNode]\n",
    "        if len(nodeList2)==0:\n",
    "            nodesToCheck = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensure that all cells with glacial wells are glacial sediments. Set the surf geo to 5 if it is >7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid.loc[(model_grid.well_Glacial==1)&(model_grid.SurfGeo>7),\"SurfGeo\"]=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.04634385  0.71807291  5.82499981]\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add observation type to cells.  \"hydro\" if a perennial stream, \"topo\" everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_active = model_grid.ibound != 0\n",
    "is_intermit = model_grid.reach_intermit\n",
    "is_sea = model_grid.ghb_sea ==1\n",
    "#marine cells (those with ghb_sea boundary) are included as hydro observationgs\n",
    "hydro_index = model_grid.stage.notnull() | is_sea\n",
    "topo_index = model_grid.stage.isnull()\n",
    "\n",
    "is_hydro_obs = is_active & ~is_intermit & hydro_index\n",
    "is_topo_obs = is_active & ~is_hydro_obs & ~ is_sea\n",
    "\n",
    "model_grid.loc[is_hydro_obs, 'obs_type'] = 'hydro'\n",
    "model_grid.loc[is_topo_obs, 'obs_type'] = 'topo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add distance to nearest stream cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delr = np.ones((NCOL)) * L\n",
    "delc = np.ones((NROW)) * L\n",
    "grid_len_y = L * NROW\n",
    "x = np.cumsum(delr) - delr / 2 \n",
    "y = grid_len_y - (np.cumsum(delc) - delc / 2 )\n",
    "xc, yc = np.meshgrid(x, y)\n",
    "model_grid['xc'] = xc.ravel()\n",
    "model_grid['yc'] = yc.ravel()\n",
    "\n",
    "stream_cells = model_grid.loc[is_hydro_obs, ['xc', 'yc']]\n",
    "topo_cells = model_grid.loc[is_topo_obs, ['xc', 'yc']]\n",
    "\n",
    "try:\n",
    "    tmp = ss.distance.cdist(topo_cells, stream_cells)\n",
    "    ctmp = tmp.min(axis=1)\n",
    "\n",
    "    topo_cells['dist2str'] = ctmp\n",
    "    model_grid = model_grid.join(topo_cells, how='outer', lsuffix='r_', rsuffix='l_')\n",
    "    model_grid.loc[model_grid.obs_type == 'hydro', ['dist2str']] = np.sqrt(L ** 2 / ( 4 * np.pi ))\n",
    "except (ValueError, MemoryError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delr = np.ones((NCOL)) * L\n",
    "delc = np.ones((NROW)) * L\n",
    "grid_len_y = L * NROW\n",
    "x = np.cumsum(delr) - delr / 2 \n",
    "y = grid_len_y - (np.cumsum(delc) - delc / 2 )\n",
    "xc, yc = np.meshgrid(x, y)\n",
    "model_grid['xc'] = xc.ravel()\n",
    "model_grid['yc'] = yc.ravel()\n",
    "road_cells = model_grid.loc[model_grid.road==1, ['xc', 'yc']]\n",
    "nonRoadCells = model_grid.loc[model_grid.road==0,['xc','yc']]\n",
    "\n",
    "\n",
    "try:\n",
    "    tmp = ss.distance.cdist(nonRoadCells, road_cells)\n",
    "    ctmp = tmp.min(axis=1)\n",
    "\n",
    "    nonRoadCells['dist2road'] = ctmp\n",
    "    model_grid = model_grid.join(nonRoadCells, how='outer', lsuffix='r_', rsuffix='l_')\n",
    "    model_grid.loc[model_grid.road==1, ['dist2road']] = 0\n",
    "except (ValueError, MemoryError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_grid_cols = [u'segment_len', u'reachcode', u'reach_len', u'reach_int', u'obs_type',\n",
    "                u'lay', u'xcl_', u'ycl_', u'xcr_', u'ycr_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the columns in model_grid and write a GeoTiff of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_num\n",
      "ibound\n",
      "SHEDS\n",
      "arbolateSu\n",
      "gess_poly\n",
      "lake\n",
      "kauffman_CTquat_thk\n",
      "HUC12_shortCD\n",
      "HUC12_shortCD_ext\n",
      "sewer\n",
      "pws\n",
      "well_PWSFlux\n",
      "well_Glacial\n",
      "BedLithSimp\n",
      "SurfGeo\n",
      "embayment\n",
      "baseflowGage\n",
      "ned\n",
      "ned_mean\n",
      "kauffman_bedrock_el\n",
      "slope\n",
      "ned_coast_min\n",
      "catchment\n",
      "soller_thk\n",
      "soller_bedrock_el\n",
      "nlcd\n",
      "clear\n",
      "clear_per_grass\n",
      "nlcd_ag\n",
      "WetlandPer\n",
      "PerDev\n",
      "PerImp\n",
      "population\n",
      "rch_eff_m_SWB_NAWQA\n",
      "rch_eff_m_Reitz_2013\n",
      "rch_m_Wolock\n",
      "Coast_Deep\n",
      "Coast_Shallow\n",
      "PerWater\n",
      "PerSand\n",
      "PerClay\n",
      "PerSilt\n",
      "NDep_dry\n",
      "NDep_wet\n",
      "edge\n",
      "ghb\n",
      "stage\n",
      "order\n",
      "arbolateSum\n",
      "reach_intermit\n",
      "riv_coast\n",
      "road\n",
      "road_length\n",
      "ghb_sea\n",
      "fresh_head\n",
      "Coast_all\n",
      "Coast_major\n",
      "top\n",
      "row\n",
      "col\n",
      "dist2str\n",
      "dist2road\n"
     ]
    }
   ],
   "source": [
    "for column, item in model_grid.iteritems():\n",
    "    if column not in no_grid_cols:\n",
    "        print(column)\n",
    "        fname = '{}.tif'.format(column)\n",
    "        dst = os.path.join(model_ws, fname)\n",
    "        if os.path.exists(dst):\n",
    "            os.remove(dst)\n",
    "        data = item.values.reshape(NROW,NCOL)\n",
    "        make_raster(dst, data, NCOL, NROW, gt, shapeproj, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write model_grid to model_grid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_file = os.path.join(model_ws, 'model_grid')\n",
    "model_file = '{}.csv'.format(model_file)\n",
    "model_grid.to_csv(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a quick model to export the model grid as a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote model_grid.shp\n"
     ]
    }
   ],
   "source": [
    "ml = fp.modflow.Modflow(modelname=\"temp\", exe_name=mfpth, version='mfnwt') \n",
    "\n",
    "# add packages (DIS has to come before either BAS or the flow package)\n",
    "dis = fp.modflow.ModflowDis(ml, nlay=1, nrow=NROW, ncol=NCOL, nper=1, delr=L, delc=L, \n",
    "                                laycbd=0, top=1, botm=0, perlen=1.E+05, nstp=1, tsmult=1, \n",
    "                                steady=True, itmuni=4, lenuni=2, extension='dis', \n",
    "                                unitnumber=11)\n",
    "bas = fp.modflow.ModflowBas(ml, ibound=model_grid.ibound.values.reshape(NROW, NCOL), strt=1, ifrefm=True, \n",
    "                                ixsec=False, ichflg=False, stoper=None, hnoflo=hnoflo, extension='bas', \n",
    "                                unitnumber=13)\n",
    "#ml.sr = SpatialReference(rotation = theta*180 / np.pi, delr=delr, delc=delc, xul=origin[0], yul=origin[1],  proj4_str=\"EPSG:5070\")\n",
    "ml.sr = fp.utils.reference.SpatialReference(delr=delr, delc=delc, xul = origin[0], yul = origin[1], rotation = theta*180 / np.pi, units=\"meters\", proj4_str=\"EPSG:5070\")\n",
    "other_dict={}\n",
    "other_dict[\"node\"]=np.asarray([x for x in range(model_grid.shape[0])]).reshape(dis.nrow, dis.ncol)\n",
    "other_dict[\"Coast_Deep\"]=model_grid.Coast_Deep.values.reshape(dis.nrow, dis.ncol)\n",
    "other_dict[\"Coast_Shallow\"]=model_grid.Coast_Shallow.values.reshape(dis.nrow, dis.ncol)\n",
    "\n",
    "#use the prj file from the domain file\n",
    "prj = domain_file.replace(\"shp\",\"prj\")\n",
    "\n",
    "#temporarily change the directory\n",
    "thisDir = os.getcwd()\n",
    "os.chdir(model_ws)\n",
    "fp.export.shapefile_utils.model_attributes_to_shapefile(\"model_grid.shp\",ml,array_dict=other_dict, prj=prj)\n",
    "\n",
    "#change the directory back\n",
    "os.chdir(thisDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the gages within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gage_loc_file = os.path.join(nhd_dir,\"NHDPlusNationalData\",\"GageLoc.shp\")\n",
    "gageDF = gp.read_file(gage_loc_file)\n",
    "gageDF['Long'] = gageDF.geometry.apply(lambda p: p.x)\n",
    "gageDF['Lat'] = gageDF.geometry.apply(lambda p: p.y)\n",
    "gageDF = gageDF.to_crs(crs=prj4)\n",
    "gageModel = gageDF['geometry'].intersection(prj_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add fields from gageDF\n",
    "gageModel = pd.DataFrame(gageModel, columns=['geometry'])\n",
    "gageModel['REACHCODE'] = gageDF.REACHCODE\n",
    "gageModel['COMID'] = gageDF.FLComID\n",
    "gageModel['REACHCODE'] = gageDF.REACHCODE\n",
    "gageModel['site_no'] = gageDF.SOURCE_FEA\n",
    "gageModel['Measure'] = gageDF.Measure\n",
    "gageModel['Lat']=gageDF.Lat\n",
    "gageModel['Long']=gageDF.Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add other gage info\n",
    "gage_tab = dbf2df(os.path.join(nhd_dir,\"NHDPlusNationalData\",\"GageInfo.dbf\"), cols=['GAGEID', 'STATION_NM','GagesII'])\n",
    "gageModel = gageModel.merge(gage_tab, how='left', left_on='site_no', right_on='GAGEID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove gages outside of the model area\n",
    "gageModel = gp.GeoDataFrame(gageModel, crs=prj4, geometry=gageModel.geometry)\n",
    "gageModel = gageModel[-gageModel.geometry.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the model gages as a shapefile and csv\n",
    "gageModel.to_file(os.path.join(model_ws, 'ModelGages.shp'))\n",
    "gageModel[gageModel.columns[1:]].to_csv(os.path.join(model_ws, 'ModelGages.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1955ff32ac8>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAD8CAYAAABXctsfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1955fdb51d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ma = np.ma.MaskedArray(model_grid.ned.values.reshape(NROW,NCOL), mask=(ib==0))\n",
    "plt.imshow(model_grid.ned.values.reshape(NROW,NCOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_grid_copy = deepcopy(model_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_grid = deepcopy(model_grid_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_grid['population_float']=model_grid.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9650.4131"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model_grid.population_float[model_grid.ibound==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.32258532e-02,   2.03226209e+00,   5.62581778e+00,\n",
       "         1.12616358e+01,   1.61125488e+02])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(model_grid.population[(model_grid.population>0)&(model_grid.ibound==1)],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.32258532e-02,   2.03226209e+00,   5.62581778e+00,\n",
       "         1.12616358e+01,   1.61125488e+02])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(model_grid.population_float[(model_grid.population_float>0)&(model_grid.ibound==1)],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9650.4131"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model_grid.population[model_grid.ibound==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "251\n",
      "120.36164546012878\n"
     ]
    }
   ],
   "source": [
    "#first, get a list of cells with non-zero populations below the threshold\n",
    "import time\n",
    "startTime = time.time()\n",
    "\n",
    "lowPop = (model_grid.population<popThreshold)&(model_grid.ibound==1)&(model_grid.population!=0)\n",
    "lowPopCells = model_grid.loc[lowPop,[\"node_num\",\"population\",\"row\",\"col\",\"road\",\"road_length\"]]\n",
    "thisCountMax = np.sum(lowPop)\n",
    "print(thisCountMax)\n",
    "for thisCount in range(thisCountMax):\n",
    "\n",
    "    if lowPopCells.shape[0]==0:\n",
    "        break\n",
    "\n",
    "    if thisCount%500==0:\n",
    "        print(lowPopCells.shape[0])\n",
    "    lowPopCellsClip = lowPopCells[lowPopCells.population==np.min(lowPopCells.population)]\n",
    "    lowPopCellsClip = lowPopCellsClip.sort_values(by=['population','road','road_length'])\n",
    "    thisRow = lowPopCellsClip.row.values[0]\n",
    "    thisCol = lowPopCellsClip.col.values[0]\n",
    "    #get a cluster of low population cells, with the initial cluster size based on the grid resolution\n",
    "    ncells = 0\n",
    "    if L==500*ft2m:\n",
    "        ncells = 1\n",
    "    elif L==250*ft2m:\n",
    "        ncells = 2\n",
    "    lowRow = thisRow-ncells\n",
    "    highRow = thisRow+ncells\n",
    "    lowCol = thisCol-ncells\n",
    "    highCol = thisCol+ncells\n",
    "    cellFlag = (lowPopCells.row>=lowRow)&(lowPopCells.row<=highRow)&(lowPopCells.col>=lowCol)&(lowPopCells.col<=highCol)\n",
    "    thisCell = lowPopCells[cellFlag]\n",
    "    \n",
    "    #ensure that the sum of the population from the cluster is < the threshold\n",
    "    while np.sum(thisCell.population)>popThreshold:\n",
    "        ncells = ncells - 1\n",
    "        thisCell = lowPopCells.loc[(lowPopCells.row>=(thisRow-ncells))&(lowPopCells.row<=(thisRow+ncells))&(lowPopCells.col>=(thisCol-ncells))&(lowPopCells.col<=(thisCol+ncells))]\n",
    "\n",
    "    for thisRange in range(6):\n",
    "        lowRow = thisRow-(thisRange+ncells)\n",
    "        highRow = thisRow+(thisRange+ncells)\n",
    "        lowCol = thisCol-(thisRange+ncells)\n",
    "        highCol = thisCol+(thisRange+ncells)\n",
    "        nearbyCells = model_grid.loc[~(model_grid.node_num.isin(thisCell.node_num))&(model_grid.row>=lowRow)&(model_grid.row<=highRow)&(model_grid.col>=lowCol)&(model_grid.col<=highCol)&(model_grid.population!=0),[\"node_num\",\"population\",\"row\",\"col\",\"road\",\"road_length\"]]\n",
    "        \n",
    "        if nearbyCells.shape[0]==0:\n",
    "            continue\n",
    "\n",
    "        nearbyCells['Distance']=np.sqrt(np.abs(nearbyCells.row-thisRow)^2+np.abs(nearbyCells.col-thisCol)^2)\n",
    "\n",
    "        nearbyCells=nearbyCells.sort_values(by=['road','population','road_length','Distance'],ascending=False)\n",
    "        newNode = nearbyCells.node_num.values[0]\n",
    "\n",
    "        #copy the current cell's population to the new cells population\n",
    "        thisPop = np.sum(thisCell.population)\n",
    "        model_grid.loc[model_grid.node_num==newNode,\"population\"]=model_grid.loc[model_grid.node_num==newNode,\"population\"]+thisPop\n",
    "        lowPopCells.loc[lowPopCells.node_num==newNode,\"population\"]=model_grid.loc[model_grid.node_num==newNode,\"population\"]\n",
    " \n",
    "        #set the current cell's population to 0\n",
    "        model_grid.loc[model_grid.node_num.isin(thisCell.node_num.values),\"population\"]=0\n",
    "        break\n",
    "\n",
    "    cellsToRemove = [x for x in thisCell.node_num.values]\n",
    "    if model_grid.loc[model_grid.node_num==newNode,\"population\"].values[0]>popThreshold:\n",
    "        cellsToRemove.append(newNode)\n",
    "    lowPopCells = lowPopCells[~lowPopCells.node_num.isin(cellsToRemove)]\n",
    "    lowPop = (lowPopCells.population<popThreshold)&(lowPopCells.population!=0)\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " no_grid_cols = [x for x in model_grid.columns if not x.startswith(\"population\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column, item in model_grid.iteritems():\n",
    "    if column not in no_grid_cols:\n",
    "        fname = '{}.tif'.format(column)\n",
    "        dst = os.path.join(model_ws, fname)\n",
    "        if os.path.exists(dst):\n",
    "            os.remove(dst)\n",
    "        data = item.values.reshape(NROW,NCOL)\n",
    "        make_raster(dst, data, NCOL, NROW, gt, shapeproj, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['node_num', 'ibound', 'SHEDS', 'arbolateSu', 'gess_poly', 'lake',\n",
       "       'kauffman_CTquat_thk', 'HUC12_shortCD', 'HUC12_shortCD_ext', 'sewer',\n",
       "       'pws', 'well_PWSFlux', 'well_Glacial', 'BedLithSimp', 'SurfGeo',\n",
       "       'embayment', 'baseflowGage', 'ned', 'ned_mean', 'kauffman_bedrock_el',\n",
       "       'slope', 'ned_coast_min', 'catchment', 'soller_thk',\n",
       "       'soller_bedrock_el', 'nlcd', 'clear', 'clear_per_grass', 'nlcd_ag',\n",
       "       'WetlandPer', 'PerDev', 'PerImp', 'population', 'rch_eff_m_SWB_NAWQA',\n",
       "       'rch_eff_m_Reitz_2013', 'rch_m_Wolock', 'Coast_Deep', 'Coast_Shallow',\n",
       "       'PerWater', 'PerSand', 'PerClay', 'PerSilt', 'NDep_dry', 'NDep_wet',\n",
       "       'edge', 'ghb', 'stage', 'segment_len', 'order', 'arbolateSum',\n",
       "       'reachcode', 'reach_intermit', 'reach_len', 'reach_int', 'riv_coast',\n",
       "       'road', 'road_length', 'ghb_sea', 'fresh_head', 'Coast_all',\n",
       "       'Coast_major', 'top', 'lay', 'row', 'col', 'obs_type', 'xcr_', 'ycr_',\n",
       "       'xcl_', 'ycl_', 'dist2str', 'xcr_', 'ycr_', 'xcl_', 'ycl_', 'dist2road',\n",
       "       'population_float'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the model grid with the updated populations\n",
    "model_grid.to_csv(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.percentile(model_grid.fresh_head[model_grid.ghb_sea==1],[0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(model_grid.Coast_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {
    "645566330510416199c0bb6ae8c58f5a": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
